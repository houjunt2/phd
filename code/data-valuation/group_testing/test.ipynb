{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvxpy import *\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "x = iris.data\n",
    "y = iris.target\n",
    "x = x[np.where(y!=2)[0],:]\n",
    "x_mean = np.linalg.norm(x,ord=2,axis=1)\n",
    "x = x/np.max(x_mean)\n",
    "y = y[np.where(y!=2)[0]]\n",
    "y = 2*y-1 # change the label (0,1) to (-1,1)\n",
    "permute_ind = np.random.permutation(x.shape[0])\n",
    "x = x[permute_ind,:]\n",
    "y = y[permute_ind]\n",
    "n_all = x.shape[0]\n",
    "n_trn = 40\n",
    "n_tst = n_trn\n",
    "x_trn,y_trn = x[:n_trn,:],y[:n_trn]\n",
    "x_tst,y_tst = x[n_trn:n_trn+n_tst,:],y[n_trn:n_trn+n_tst]\n",
    "\n",
    "\n",
    "w = Variable((x_trn.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "(4, 1)\n",
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "print(w.value)\n",
    "print((x_trn.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "438824.2604559979"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "## logistic regression parameters\n",
    "regularizer = 0.01\n",
    "max_loss = np.log(2) # range of the uility\n",
    "epsilon = 1/(n_trn)\n",
    "delta = 0.05\n",
    "\n",
    "\n",
    "## group testing parameters\n",
    "r = max_loss # range of utility\n",
    "N = n_trn\n",
    "Z = 2*np.sum([1/i for i in range(1,N)])\n",
    "T = int(np.ceil(2*(r**2)*(Z**2)*np.log(N*(N-1)/delta)/(epsilon**2)))\n",
    "\n",
    "def h(x):\n",
    "    y = (1+x)*np.log(1+x) - x\n",
    "    return y\n",
    "\n",
    "q = [1 / Z * (1 / k + 1 / (N - k)) for k in range(1, N)]\n",
    "q_tot = q[0]\n",
    "for j in range(1, N-1):\n",
    "    k = j + 1\n",
    "    q_tot += q[j] * (1 + 2 * k * (k - N) / (N*(N-1)))\n",
    "T_new = 4/(1-q_tot**2)/h(2*epsilon/Z/r/(1-q_tot**2))*np.log(N*(N-1)/(2*delta))\n",
    "T_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 4)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tst.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1099511627776"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utility import estimate_testing_loss_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\666\\anaconda3\\envs\\python\\lib\\site-packages\\cvxpy\\expressions\\expression.py:650: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 417 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\666\\anaconda3\\envs\\python\\lib\\site-packages\\cvxpy\\expressions\\expression.py:650: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 418 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\666\\anaconda3\\envs\\python\\lib\\site-packages\\cvxpy\\expressions\\expression.py:650: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 419 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\666\\anaconda3\\envs\\python\\lib\\site-packages\\cvxpy\\expressions\\expression.py:650: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 420 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\666\\anaconda3\\envs\\python\\lib\\site-packages\\cvxpy\\expressions\\expression.py:650: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 421 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\666\\anaconda3\\envs\\python\\lib\\site-packages\\cvxpy\\expressions\\expression.py:650: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 422 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\666\\anaconda3\\envs\\python\\lib\\site-packages\\cvxpy\\expressions\\expression.py:650: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 423 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\666\\anaconda3\\envs\\python\\lib\\site-packages\\cvxpy\\expressions\\expression.py:650: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 424 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\666\\anaconda3\\envs\\python\\lib\\site-packages\\cvxpy\\expressions\\expression.py:650: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 425 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\666\\anaconda3\\envs\\python\\lib\\site-packages\\cvxpy\\expressions\\expression.py:650: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 426 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\666\\anaconda3\\envs\\python\\lib\\site-packages\\cvxpy\\expressions\\expression.py:650: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 427 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\666\\anaconda3\\envs\\python\\lib\\site-packages\\cvxpy\\expressions\\expression.py:650: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 428 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\666\\anaconda3\\envs\\python\\lib\\site-packages\\cvxpy\\expressions\\expression.py:650: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 429 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.27806737098385725,\n",
       " array([-1.04584319, -2.82275386,  3.53066315,  1.42286472]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "num_active_users = np.random.choice(np.arange(1, N), 1, False, q)\n",
    "active_users_ind = np.random.choice(np.arange(N), num_active_users, False)\n",
    "A_t = np.zeros(N)\n",
    "A_t[active_users_ind] = 1\n",
    "x_trn_active = x_trn[active_users_ind, :]\n",
    "y_trn_active = y_trn[active_users_ind]\n",
    "estimate_testing_loss_weight(x_trn_active, y_trn_active, x_tst, y_tst, regularizer, max_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Loss = 0.11939961789245597\n",
      "Iteration 1: Loss = 0.11879792333369558\n",
      "Iteration 2: Loss = 0.11820278996176505\n",
      "Iteration 3: Loss = 0.11761414602298054\n",
      "Iteration 4: Loss = 0.11703192054872805\n",
      "Iteration 5: Loss = 0.11645604334687307\n",
      "Iteration 6: Loss = 0.11588644499326439\n",
      "Iteration 7: Loss = 0.11532305682333087\n",
      "Iteration 8: Loss = 0.11476581092377007\n",
      "Iteration 9: Loss = 0.11421464012432782\n",
      "Iteration 10: Loss = 0.11366947798966798\n",
      "Iteration 11: Loss = 0.11313025881133078\n",
      "Iteration 12: Loss = 0.11259691759977944\n",
      "Iteration 13: Loss = 0.11206939007653376\n",
      "Iteration 14: Loss = 0.11154761266638963\n",
      "Iteration 15: Loss = 0.11103152248972392\n",
      "Iteration 16: Loss = 0.11052105735488339\n",
      "Iteration 17: Loss = 0.11001615575065697\n",
      "Iteration 18: Loss = 0.10951675683883032\n",
      "Iteration 19: Loss = 0.10902280044682193\n",
      "Iteration 20: Loss = 0.10853422706039988\n",
      "Iteration 21: Loss = 0.10805097781647806\n",
      "Iteration 22: Loss = 0.10757299449599143\n",
      "Iteration 23: Loss = 0.10710021951684918\n",
      "Iteration 24: Loss = 0.10663259592696489\n",
      "Iteration 25: Loss = 0.10617006739736315\n",
      "Iteration 26: Loss = 0.1057125782153614\n",
      "Iteration 27: Loss = 0.1052600732778266\n",
      "Iteration 28: Loss = 0.10481249808450543\n",
      "Iteration 29: Loss = 0.10436979873142771\n",
      "Iteration 30: Loss = 0.10393192190438189\n",
      "Iteration 31: Loss = 0.10349881487246178\n",
      "Iteration 32: Loss = 0.1030704254816843\n",
      "Iteration 33: Loss = 0.10264670214867665\n",
      "Iteration 34: Loss = 0.10222759385443274\n",
      "Iteration 35: Loss = 0.10181305013813799\n",
      "Iteration 36: Loss = 0.10140302109106156\n",
      "Iteration 37: Loss = 0.10099745735051549\n",
      "Iteration 38: Loss = 0.10059631009387995\n",
      "Iteration 39: Loss = 0.10019953103269387\n",
      "Iteration 40: Loss = 0.0998070724068102\n",
      "Iteration 41: Loss = 0.09941888697861499\n",
      "Iteration 42: Loss = 0.09903492802731016\n",
      "Iteration 43: Loss = 0.09865514934325827\n",
      "Iteration 44: Loss = 0.09827950522238954\n",
      "Iteration 45: Loss = 0.09790795046067004\n",
      "Iteration 46: Loss = 0.09754044034863014\n",
      "Iteration 47: Loss = 0.09717693066595323\n",
      "Iteration 48: Loss = 0.09681737767612333\n",
      "Iteration 49: Loss = 0.09646173812113146\n",
      "Iteration 50: Loss = 0.09610996921623972\n",
      "Iteration 51: Loss = 0.09576202864480297\n",
      "Iteration 52: Loss = 0.0954178745531469\n",
      "Iteration 53: Loss = 0.09507746554550224\n",
      "Iteration 54: Loss = 0.09474076067899442\n",
      "Iteration 55: Loss = 0.09440771945868819\n",
      "Iteration 56: Loss = 0.09407830183268598\n",
      "Iteration 57: Loss = 0.09375246818728047\n",
      "Iteration 58: Loss = 0.09343017934216005\n",
      "Iteration 59: Loss = 0.09311139654566634\n",
      "Iteration 60: Loss = 0.09279608147010418\n",
      "Iteration 61: Loss = 0.09248419620710246\n",
      "Iteration 62: Loss = 0.09217570326302614\n",
      "Iteration 63: Loss = 0.09187056555443814\n",
      "Iteration 64: Loss = 0.09156874640361093\n",
      "Iteration 65: Loss = 0.09127020953408747\n",
      "Iteration 66: Loss = 0.09097491906629046\n",
      "Iteration 67: Loss = 0.09068283951317968\n",
      "Iteration 68: Loss = 0.09039393577595703\n",
      "Iteration 69: Loss = 0.09010817313981835\n",
      "Iteration 70: Loss = 0.08982551726975185\n",
      "Iteration 71: Loss = 0.0895459342063824\n",
      "Iteration 72: Loss = 0.08926939036186145\n",
      "Iteration 73: Loss = 0.08899585251580183\n",
      "Iteration 74: Loss = 0.08872528781125695\n",
      "Iteration 75: Loss = 0.08845766375074422\n",
      "Iteration 76: Loss = 0.08819294819231174\n",
      "Iteration 77: Loss = 0.08793110934564821\n",
      "Iteration 78: Loss = 0.08767211576823528\n",
      "Iteration 79: Loss = 0.08741593636154205\n",
      "Iteration 80: Loss = 0.08716254036726127\n",
      "Iteration 81: Loss = 0.08691189736358668\n",
      "Iteration 82: Loss = 0.08666397726153094\n",
      "Iteration 83: Loss = 0.08641875030128417\n",
      "Iteration 84: Loss = 0.08617618704861191\n",
      "Iteration 85: Loss = 0.08593625839129305\n",
      "Iteration 86: Loss = 0.08569893553559613\n",
      "Iteration 87: Loss = 0.08546419000279455\n",
      "Iteration 88: Loss = 0.08523199362572001\n",
      "Iteration 89: Loss = 0.08500231854535327\n",
      "Iteration 90: Loss = 0.08477513720745261\n",
      "Iteration 91: Loss = 0.08455042235921888\n",
      "Iteration 92: Loss = 0.08432814704599734\n",
      "Iteration 93: Loss = 0.08410828460801521\n",
      "Iteration 94: Loss = 0.08389080867715538\n",
      "Iteration 95: Loss = 0.08367569317376497\n",
      "Iteration 96: Loss = 0.08346291230349906\n",
      "Iteration 97: Loss = 0.08325244055419898\n",
      "Iteration 98: Loss = 0.08304425269280469\n",
      "Iteration 99: Loss = 0.08283832376230081\n",
      "Iteration 100: Loss = 0.08263462907869636\n",
      "Iteration 101: Loss = 0.08243314422803731\n",
      "Iteration 102: Loss = 0.08223384506345195\n",
      "Iteration 103: Loss = 0.08203670770222839\n",
      "Iteration 104: Loss = 0.08184170852292443\n",
      "Iteration 105: Loss = 0.08164882416250854\n",
      "Iteration 106: Loss = 0.0814580315135327\n",
      "Iteration 107: Loss = 0.08126930772133555\n",
      "Iteration 108: Loss = 0.08108263018127668\n",
      "Iteration 109: Loss = 0.08089797653600087\n",
      "Iteration 110: Loss = 0.08071532467273242\n",
      "Iteration 111: Loss = 0.08053465272059895\n",
      "Iteration 112: Loss = 0.08035593904798465\n",
      "Iteration 113: Loss = 0.08017916225991228\n",
      "Iteration 114: Loss = 0.08000430119545414\n",
      "Iteration 115: Loss = 0.07983133492517083\n",
      "Iteration 116: Loss = 0.0796602427485788\n",
      "Iteration 117: Loss = 0.07949100419164477\n",
      "Iteration 118: Loss = 0.07932359900430828\n",
      "Iteration 119: Loss = 0.07915800715803087\n",
      "Iteration 120: Loss = 0.0789942088433723\n",
      "Iteration 121: Loss = 0.07883218446759332\n",
      "Iteration 122: Loss = 0.07867191465228447\n",
      "Iteration 123: Loss = 0.07851338023102104\n",
      "Iteration 124: Loss = 0.07835656224704356\n",
      "Iteration 125: Loss = 0.0782014419509637\n",
      "Iteration 126: Loss = 0.07804800079849536\n",
      "Iteration 127: Loss = 0.07789622044821039\n",
      "Iteration 128: Loss = 0.07774608275931912\n",
      "Iteration 129: Loss = 0.0775975697894749\n",
      "Iteration 130: Loss = 0.07745066379260292\n",
      "Iteration 131: Loss = 0.07730534721675258\n",
      "Iteration 132: Loss = 0.07716160270197361\n",
      "Iteration 133: Loss = 0.07701941307821503\n",
      "Iteration 134: Loss = 0.07687876136324757\n",
      "Iteration 135: Loss = 0.07673963076060843\n",
      "Iteration 136: Loss = 0.0766020046575688\n",
      "Iteration 137: Loss = 0.07646586662312345\n",
      "Iteration 138: Loss = 0.07633120040600243\n",
      "Iteration 139: Loss = 0.07619798993270442\n",
      "Iteration 140: Loss = 0.07606621930555167\n",
      "Iteration 141: Loss = 0.07593587280076627\n",
      "Iteration 142: Loss = 0.07580693486656734\n",
      "Iteration 143: Loss = 0.0756793901212891\n",
      "Iteration 144: Loss = 0.07555322335151962\n",
      "Iteration 145: Loss = 0.07542841951025976\n",
      "Iteration 146: Loss = 0.07530496371510242\n",
      "Iteration 147: Loss = 0.07518284124643163\n",
      "Iteration 148: Loss = 0.07506203754564134\n",
      "Iteration 149: Loss = 0.07494253821337374\n",
      "Iteration 150: Loss = 0.07482432900777682\n",
      "Iteration 151: Loss = 0.07470739584278104\n",
      "Iteration 152: Loss = 0.0745917247863948\n",
      "Iteration 153: Loss = 0.07447730205901859\n",
      "Iteration 154: Loss = 0.07436411403177765\n",
      "Iteration 155: Loss = 0.07425214722487272\n",
      "Iteration 156: Loss = 0.07414138830594895\n",
      "Iteration 157: Loss = 0.07403182408848273\n",
      "Iteration 158: Loss = 0.0739234415301859\n",
      "Iteration 159: Loss = 0.07381622773142779\n",
      "Iteration 160: Loss = 0.07371016993367416\n",
      "Iteration 161: Loss = 0.07360525551794357\n",
      "Iteration 162: Loss = 0.07350147200328025\n",
      "Iteration 163: Loss = 0.07339880704524415\n",
      "Iteration 164: Loss = 0.07329724843441701\n",
      "Iteration 165: Loss = 0.07319678409492522\n",
      "Iteration 166: Loss = 0.0730974020829785\n",
      "Iteration 167: Loss = 0.07299909058542482\n",
      "Iteration 168: Loss = 0.07290183791832097\n",
      "Iteration 169: Loss = 0.07280563252551892\n",
      "Iteration 170: Loss = 0.07271046297726734\n",
      "Iteration 171: Loss = 0.07261631796882893\n",
      "Iteration 172: Loss = 0.07252318631911236\n",
      "Iteration 173: Loss = 0.07243105696931955\n",
      "Iteration 174: Loss = 0.0723399189816075\n",
      "Iteration 175: Loss = 0.07224976153776497\n",
      "Iteration 176: Loss = 0.07216057393790348\n",
      "Iteration 177: Loss = 0.07207234559916273\n",
      "Iteration 178: Loss = 0.07198506605443009\n",
      "Iteration 179: Loss = 0.07189872495107427\n",
      "Iteration 180: Loss = 0.07181331204969263\n",
      "Iteration 181: Loss = 0.07172881722287233\n",
      "Iteration 182: Loss = 0.07164523045396512\n",
      "Iteration 183: Loss = 0.07156254183587525\n",
      "Iteration 184: Loss = 0.07148074156986094\n",
      "Iteration 185: Loss = 0.0713998199643488\n",
      "Iteration 186: Loss = 0.0713197674337613\n",
      "Iteration 187: Loss = 0.07124057449735698\n",
      "Iteration 188: Loss = 0.07116223177808349\n",
      "Iteration 189: Loss = 0.07108473000144298\n",
      "Iteration 190: Loss = 0.07100805999437015\n",
      "Iteration 191: Loss = 0.07093221268412238\n",
      "Iteration 192: Loss = 0.07085717909718202\n",
      "Iteration 193: Loss = 0.0707829503581709\n",
      "Iteration 194: Loss = 0.07070951768877638\n",
      "Iteration 195: Loss = 0.07063687240668946\n",
      "Iteration 196: Loss = 0.07056500592455436\n",
      "Iteration 197: Loss = 0.07049390974892954\n",
      "Iteration 198: Loss = 0.07042357547926022\n",
      "Iteration 199: Loss = 0.07035399480686215\n",
      "Iteration 200: Loss = 0.07028515951391633\n",
      "Iteration 201: Loss = 0.07021706147247482\n",
      "Iteration 202: Loss = 0.0701496926434776\n",
      "Iteration 203: Loss = 0.07008304507577988\n",
      "Iteration 204: Loss = 0.07001711090519025\n",
      "Iteration 205: Loss = 0.06995188235351933\n",
      "Iteration 206: Loss = 0.0698873517276388\n",
      "Iteration 207: Loss = 0.06982351141855071\n",
      "Iteration 208: Loss = 0.06976035390046705\n",
      "Iteration 209: Loss = 0.06969787172989926\n",
      "Iteration 210: Loss = 0.0696360575447579\n",
      "Iteration 211: Loss = 0.06957490406346194\n",
      "Iteration 212: Loss = 0.06951440408405794\n",
      "Iteration 213: Loss = 0.0694545504833489\n",
      "Iteration 214: Loss = 0.06939533621603251\n",
      "Iteration 215: Loss = 0.06933675431384892\n",
      "Iteration 216: Loss = 0.06927879788473783\n",
      "Iteration 217: Loss = 0.06922146011200477\n",
      "Iteration 218: Loss = 0.06916473425349652\n",
      "Iteration 219: Loss = 0.06910861364078558\n",
      "Iteration 220: Loss = 0.06905309167836349\n",
      "Iteration 221: Loss = 0.06899816184284306\n",
      "Iteration 222: Loss = 0.0689438176821693\n",
      "Iteration 223: Loss = 0.0688900528148389\n",
      "Iteration 224: Loss = 0.06883686092912843\n",
      "Iteration 225: Loss = 0.06878423578233087\n",
      "Iteration 226: Loss = 0.06873217120000033\n",
      "Iteration 227: Loss = 0.06868066107520553\n",
      "Iteration 228: Loss = 0.06862969936779081\n",
      "Iteration 229: Loss = 0.06857928010364575\n",
      "Iteration 230: Loss = 0.06852939737398246\n",
      "Iteration 231: Loss = 0.06848004533462101\n",
      "Iteration 232: Loss = 0.0684312182052824\n",
      "Iteration 233: Loss = 0.06838291026888961\n",
      "Iteration 234: Loss = 0.06833511587087596\n",
      "Iteration 235: Loss = 0.06828782941850133\n",
      "Iteration 236: Loss = 0.06824104538017559\n",
      "Iteration 237: Loss = 0.06819475828478966\n",
      "Iteration 238: Loss = 0.06814896272105381\n",
      "Iteration 239: Loss = 0.06810365333684312\n",
      "Iteration 240: Loss = 0.06805882483855022\n",
      "Iteration 241: Loss = 0.06801447199044507\n",
      "Iteration 242: Loss = 0.06797058961404166\n",
      "Iteration 243: Loss = 0.06792717258747184\n",
      "Iteration 244: Loss = 0.0678842158448658\n",
      "Iteration 245: Loss = 0.06784171437573938\n",
      "Iteration 246: Loss = 0.06779966322438824\n",
      "Iteration 247: Loss = 0.0677580574892884\n",
      "Iteration 248: Loss = 0.06771689232250347\n",
      "Iteration 249: Loss = 0.0676761629290985\n",
      "Iteration 250: Loss = 0.06763586456656001\n",
      "Iteration 251: Loss = 0.06759599254422245\n",
      "Iteration 252: Loss = 0.067556542222701\n",
      "Iteration 253: Loss = 0.06751750901333049\n",
      "Iteration 254: Loss = 0.06747888837761057\n",
      "Iteration 255: Loss = 0.06744067582665678\n",
      "Iteration 256: Loss = 0.06740286692065778\n",
      "Iteration 257: Loss = 0.0673654572683385\n",
      "Iteration 258: Loss = 0.06732844252642899\n",
      "Iteration 259: Loss = 0.0672918183991394\n",
      "Iteration 260: Loss = 0.06725558063764041\n",
      "Iteration 261: Loss = 0.06721972503954947\n",
      "Iteration 262: Loss = 0.06718424744842266\n",
      "Iteration 263: Loss = 0.06714914375325214\n",
      "Iteration 264: Loss = 0.067114409887969\n",
      "Iteration 265: Loss = 0.06708004183095163\n",
      "Iteration 266: Loss = 0.0670460356045395\n",
      "Iteration 267: Loss = 0.06701238727455211\n",
      "Iteration 268: Loss = 0.06697909294981338\n",
      "Iteration 269: Loss = 0.06694614878168112\n",
      "Iteration 270: Loss = 0.06691355096358176\n",
      "Iteration 271: Loss = 0.06688129573055\n",
      "Iteration 272: Loss = 0.06684937935877375\n",
      "Iteration 273: Loss = 0.06681779816514373\n",
      "Iteration 274: Loss = 0.06678654850680832\n",
      "Iteration 275: Loss = 0.06675562678073303\n",
      "Iteration 276: Loss = 0.06672502942326496\n",
      "Iteration 277: Loss = 0.06669475290970191\n",
      "Iteration 278: Loss = 0.06666479375386625\n",
      "Iteration 279: Loss = 0.06663514850768353\n",
      "Iteration 280: Loss = 0.06660581376076548\n",
      "Iteration 281: Loss = 0.06657678613999794\n",
      "Iteration 282: Loss = 0.06654806230913278\n",
      "Iteration 283: Loss = 0.0665196389683849\n",
      "Iteration 284: Loss = 0.06649151285403304\n",
      "Iteration 285: Loss = 0.06646368073802543\n",
      "Iteration 286: Loss = 0.0664361394275895\n",
      "Iteration 287: Loss = 0.06640888576484591\n",
      "Iteration 288: Loss = 0.0663819166264268\n",
      "Iteration 289: Loss = 0.06635522892309834\n",
      "Iteration 290: Loss = 0.06632881959938713\n",
      "Iteration 291: Loss = 0.06630268563321108\n",
      "Iteration 292: Loss = 0.06627682403551391\n",
      "Iteration 293: Loss = 0.06625123184990397\n",
      "Iteration 294: Loss = 0.06622590615229686\n",
      "Iteration 295: Loss = 0.06620084405056197\n",
      "Iteration 296: Loss = 0.06617604268417292\n",
      "Iteration 297: Loss = 0.06615149922386189\n",
      "Iteration 298: Loss = 0.06612721087127757\n",
      "Iteration 299: Loss = 0.0661031748586469\n",
      "Iteration 300: Loss = 0.06607938844844072\n",
      "Iteration 301: Loss = 0.06605584893304271\n",
      "Iteration 302: Loss = 0.06603255363442236\n",
      "Iteration 303: Loss = 0.06600949990381105\n",
      "Iteration 304: Loss = 0.06598668512138223\n",
      "Iteration 305: Loss = 0.06596410669593457\n",
      "Iteration 306: Loss = 0.06594176206457893\n",
      "Iteration 307: Loss = 0.06591964869242858\n",
      "Iteration 308: Loss = 0.06589776407229295\n",
      "Iteration 309: Loss = 0.0658761057243745\n",
      "Iteration 310: Loss = 0.06585467119596922\n",
      "Iteration 311: Loss = 0.06583345806117011\n",
      "Iteration 312: Loss = 0.06581246392057409\n",
      "Iteration 313: Loss = 0.06579168640099209\n",
      "Iteration 314: Loss = 0.06577112315516224\n",
      "Iteration 315: Loss = 0.06575077186146622\n",
      "Iteration 316: Loss = 0.06573063022364878\n",
      "Iteration 317: Loss = 0.06571069597054031\n",
      "Iteration 318: Loss = 0.06569096685578235\n",
      "Iteration 319: Loss = 0.0656714406575562\n",
      "Iteration 320: Loss = 0.0656521151783145\n",
      "Iteration 321: Loss = 0.0656329882445157\n",
      "Iteration 322: Loss = 0.06561405770636142\n",
      "Iteration 323: Loss = 0.06559532143753682\n",
      "Iteration 324: Loss = 0.06557677733495357\n",
      "Iteration 325: Loss = 0.0655584233184959\n",
      "Iteration 326: Loss = 0.0655402573307692\n",
      "Iteration 327: Loss = 0.06552227733685154\n",
      "Iteration 328: Loss = 0.06550448132404785\n",
      "Iteration 329: Loss = 0.0654868673016467\n",
      "Iteration 330: Loss = 0.06546943330067988\n",
      "Iteration 331: Loss = 0.06545217737368456\n",
      "Iteration 332: Loss = 0.065435097594468\n",
      "Iteration 333: Loss = 0.06541819205787493\n",
      "Iteration 334: Loss = 0.06540145887955735\n",
      "Iteration 335: Loss = 0.06538489619574703\n",
      "Iteration 336: Loss = 0.0653685021630303\n",
      "Iteration 337: Loss = 0.06535227495812543\n",
      "Iteration 338: Loss = 0.06533621277766241\n",
      "Iteration 339: Loss = 0.06532031383796512\n",
      "Iteration 340: Loss = 0.06530457637483598\n",
      "Iteration 341: Loss = 0.06528899864334277\n",
      "Iteration 342: Loss = 0.06527357891760797\n",
      "Iteration 343: Loss = 0.06525831549060032\n",
      "Iteration 344: Loss = 0.06524320667392866\n",
      "Iteration 345: Loss = 0.06522825079763797\n",
      "Iteration 346: Loss = 0.06521344621000788\n",
      "Iteration 347: Loss = 0.06519879127735301\n",
      "Iteration 348: Loss = 0.06518428438382581\n",
      "Iteration 349: Loss = 0.06516992393122145\n",
      "Iteration 350: Loss = 0.06515570833878476\n",
      "Iteration 351: Loss = 0.0651416360430194\n",
      "Iteration 352: Loss = 0.06512770549749916\n",
      "Iteration 353: Loss = 0.0651139151726811\n",
      "Iteration 354: Loss = 0.06510026355572097\n",
      "Iteration 355: Loss = 0.0650867491502905\n",
      "Iteration 356: Loss = 0.06507337047639684\n",
      "Iteration 357: Loss = 0.06506012607020376\n",
      "Iteration 358: Loss = 0.065047014483855\n",
      "Iteration 359: Loss = 0.06503403428529951\n",
      "Iteration 360: Loss = 0.06502118405811849\n",
      "Iteration 361: Loss = 0.06500846240135444\n",
      "Iteration 362: Loss = 0.06499586792934206\n",
      "Iteration 363: Loss = 0.06498339927154102\n",
      "Iteration 364: Loss = 0.06497105507237044\n",
      "Iteration 365: Loss = 0.0649588339910453\n",
      "Iteration 366: Loss = 0.06494673470141467\n",
      "Iteration 367: Loss = 0.06493475589180153\n",
      "Iteration 368: Loss = 0.06492289626484457\n",
      "Iteration 369: Loss = 0.06491115453734156\n",
      "Iteration 370: Loss = 0.06489952944009447\n",
      "Iteration 371: Loss = 0.0648880197177563\n",
      "Iteration 372: Loss = 0.06487662412867966\n",
      "Iteration 373: Loss = 0.06486534144476683\n",
      "Iteration 374: Loss = 0.06485417045132158\n",
      "Iteration 375: Loss = 0.06484310994690266\n",
      "Iteration 376: Loss = 0.06483215874317878\n",
      "Iteration 377: Loss = 0.06482131566478522\n",
      "Iteration 378: Loss = 0.06481057954918198\n",
      "Iteration 379: Loss = 0.0647999492465136\n",
      "Iteration 380: Loss = 0.06478942361947035\n",
      "Iteration 381: Loss = 0.06477900154315108\n",
      "Iteration 382: Loss = 0.06476868190492742\n",
      "Iteration 383: Loss = 0.06475846360430965\n",
      "Iteration 384: Loss = 0.06474834555281386\n",
      "Iteration 385: Loss = 0.06473832667383071\n",
      "Iteration 386: Loss = 0.0647284059024955\n",
      "Iteration 387: Loss = 0.06471858218555976\n",
      "Iteration 388: Loss = 0.0647088544812642\n",
      "Iteration 389: Loss = 0.06469922175921304\n",
      "Iteration 390: Loss = 0.06468968300024976\n",
      "Iteration 391: Loss = 0.06468023719633414\n",
      "Iteration 392: Loss = 0.06467088335042073\n",
      "Iteration 393: Loss = 0.0646616204763386\n",
      "Iteration 394: Loss = 0.06465244759867235\n",
      "Iteration 395: Loss = 0.06464336375264455\n",
      "Iteration 396: Loss = 0.06463436798399942\n",
      "Iteration 397: Loss = 0.06462545934888764\n",
      "Iteration 398: Loss = 0.06461663691375269\n",
      "Iteration 399: Loss = 0.06460789975521823\n",
      "Iteration 400: Loss = 0.0645992469599767\n",
      "Iteration 401: Loss = 0.0645906776246793\n",
      "Iteration 402: Loss = 0.06458219085582706\n",
      "Iteration 403: Loss = 0.06457378576966309\n",
      "Iteration 404: Loss = 0.06456546149206607\n",
      "Iteration 405: Loss = 0.06455721715844485\n",
      "Iteration 406: Loss = 0.0645490519136343\n",
      "Iteration 407: Loss = 0.0645409649117921\n",
      "Iteration 408: Loss = 0.06453295531629696\n",
      "Iteration 409: Loss = 0.06452502229964761\n",
      "Iteration 410: Loss = 0.06451716504336319\n",
      "Iteration 411: Loss = 0.06450938273788456\n",
      "Iteration 412: Loss = 0.06450167458247673\n",
      "Iteration 413: Loss = 0.06449403978513239\n",
      "Iteration 414: Loss = 0.06448647756247645\n",
      "Iteration 415: Loss = 0.0644789871396717\n",
      "Iteration 416: Loss = 0.06447156775032531\n",
      "Iteration 417: Loss = 0.06446421863639666\n",
      "Iteration 418: Loss = 0.06445693904810587\n",
      "Iteration 419: Loss = 0.06444972824384357\n",
      "Iteration 420: Loss = 0.0644425854900815\n",
      "Iteration 421: Loss = 0.06443551006128416\n",
      "Iteration 422: Loss = 0.06442850123982141\n",
      "Iteration 423: Loss = 0.064421558315882\n",
      "Iteration 424: Loss = 0.06441468058738815\n",
      "Iteration 425: Loss = 0.06440786735991093\n",
      "Iteration 426: Loss = 0.06440111794658655\n",
      "Iteration 427: Loss = 0.06439443166803382\n",
      "Iteration 428: Loss = 0.06438780785227223\n",
      "Iteration 429: Loss = 0.06438124583464097\n",
      "Iteration 430: Loss = 0.064374744957719\n",
      "Iteration 431: Loss = 0.06436830457124584\n",
      "Iteration 432: Loss = 0.06436192403204329\n",
      "Iteration 433: Loss = 0.06435560270393798\n",
      "Iteration 434: Loss = 0.06434933995768478\n",
      "Iteration 435: Loss = 0.06434313517089109\n",
      "Iteration 436: Loss = 0.06433698772794182\n",
      "Iteration 437: Loss = 0.0643308970199254\n",
      "Iteration 438: Loss = 0.06432486244456043\n",
      "Iteration 439: Loss = 0.06431888340612316\n",
      "Iteration 440: Loss = 0.06431295931537585\n",
      "Iteration 441: Loss = 0.06430708958949584\n",
      "Iteration 442: Loss = 0.06430127365200539\n",
      "Iteration 443: Loss = 0.06429551093270232\n",
      "Iteration 444: Loss = 0.06428980086759141\n",
      "Iteration 445: Loss = 0.06428414289881657\n",
      "Iteration 446: Loss = 0.0642785364745936\n",
      "Iteration 447: Loss = 0.06427298104914397\n",
      "Iteration 448: Loss = 0.06426747608262905\n",
      "Iteration 449: Loss = 0.06426202104108518\n",
      "Iteration 450: Loss = 0.06425661539635952\n",
      "Iteration 451: Loss = 0.0642512586260464\n",
      "Iteration 452: Loss = 0.06424595021342466\n",
      "Iteration 453: Loss = 0.06424068964739528\n",
      "Iteration 454: Loss = 0.06423547642242014\n",
      "Iteration 455: Loss = 0.06423031003846105\n",
      "Iteration 456: Loss = 0.0642251900009198\n",
      "Iteration 457: Loss = 0.06422011582057847\n",
      "Iteration 458: Loss = 0.06421508701354085\n",
      "Iteration 459: Loss = 0.06421010310117407\n",
      "Iteration 460: Loss = 0.0642051636100512\n",
      "Iteration 461: Loss = 0.06420026807189426\n",
      "Iteration 462: Loss = 0.06419541602351797\n",
      "Iteration 463: Loss = 0.06419060700677406\n",
      "Iteration 464: Loss = 0.06418584056849622\n",
      "Iteration 465: Loss = 0.06418111626044562\n",
      "Iteration 466: Loss = 0.06417643363925715\n",
      "Iteration 467: Loss = 0.06417179226638593\n",
      "Iteration 468: Loss = 0.06416719170805489\n",
      "Iteration 469: Loss = 0.06416263153520242\n",
      "Iteration 470: Loss = 0.064158111323431\n",
      "Iteration 471: Loss = 0.06415363065295607\n",
      "Iteration 472: Loss = 0.06414918910855576\n",
      "Iteration 473: Loss = 0.06414478627952086\n",
      "Iteration 474: Loss = 0.06414042175960563\n",
      "Iteration 475: Loss = 0.064136095146979\n",
      "Iteration 476: Loss = 0.0641318060441762\n",
      "Iteration 477: Loss = 0.06412755405805112\n",
      "Iteration 478: Loss = 0.06412333879972917\n",
      "Iteration 479: Loss = 0.0641191598845605\n",
      "Iteration 480: Loss = 0.06411501693207379\n",
      "Iteration 481: Loss = 0.06411090956593075\n",
      "Iteration 482: Loss = 0.06410683741388076\n",
      "Iteration 483: Loss = 0.06410280010771632\n",
      "Iteration 484: Loss = 0.06409879728322883\n",
      "Iteration 485: Loss = 0.06409482858016483\n",
      "Iteration 486: Loss = 0.06409089364218286\n",
      "Iteration 487: Loss = 0.06408699211681068\n",
      "Iteration 488: Loss = 0.06408312365540296\n",
      "Iteration 489: Loss = 0.06407928791309944\n",
      "Iteration 490: Loss = 0.06407548454878367\n",
      "Iteration 491: Loss = 0.06407171322504197\n",
      "Iteration 492: Loss = 0.06406797360812302\n",
      "Iteration 493: Loss = 0.06406426536789789\n",
      "Iteration 494: Loss = 0.06406058817782034\n",
      "Iteration 495: Loss = 0.06405694171488778\n",
      "Iteration 496: Loss = 0.06405332565960249\n",
      "Iteration 497: Loss = 0.06404973969593333\n",
      "Iteration 498: Loss = 0.06404618351127786\n",
      "Iteration 499: Loss = 0.0640426567964249\n",
      "Iteration 500: Loss = 0.0640391592455174\n",
      "Iteration 501: Loss = 0.06403569055601586\n",
      "Iteration 502: Loss = 0.06403225042866213\n",
      "Iteration 503: Loss = 0.06402883856744337\n",
      "Iteration 504: Loss = 0.06402545467955678\n",
      "Iteration 505: Loss = 0.06402209847537446\n",
      "Iteration 506: Loss = 0.06401876966840866\n",
      "Iteration 507: Loss = 0.06401546797527759\n",
      "Iteration 508: Loss = 0.0640121931156714\n",
      "Iteration 509: Loss = 0.06400894481231857\n",
      "Iteration 510: Loss = 0.0640057227909529\n",
      "Iteration 511: Loss = 0.06400252678028044\n",
      "Iteration 512: Loss = 0.06399935651194724\n",
      "Iteration 513: Loss = 0.06399621172050707\n",
      "Iteration 514: Loss = 0.06399309214338969\n",
      "Iteration 515: Loss = 0.06398999752086952\n",
      "Iteration 516: Loss = 0.06398692759603444\n",
      "Iteration 517: Loss = 0.0639838821147551\n",
      "Iteration 518: Loss = 0.06398086082565457\n",
      "Iteration 519: Loss = 0.06397786348007822\n",
      "Iteration 520: Loss = 0.06397488983206395\n",
      "Iteration 521: Loss = 0.06397193963831291\n",
      "Iteration 522: Loss = 0.06396901265816021\n",
      "Iteration 523: Loss = 0.06396610865354635\n",
      "Iteration 524: Loss = 0.06396322738898863\n",
      "Iteration 525: Loss = 0.06396036863155302\n",
      "Iteration 526: Loss = 0.06395753215082638\n",
      "Iteration 527: Loss = 0.06395471771888886\n",
      "Iteration 528: Loss = 0.06395192511028673\n",
      "Iteration 529: Loss = 0.06394915410200536\n",
      "Iteration 530: Loss = 0.06394640447344267\n",
      "Iteration 531: Loss = 0.06394367600638275\n",
      "Iteration 532: Loss = 0.06394096848496973\n",
      "Iteration 533: Loss = 0.06393828169568211\n",
      "Iteration 534: Loss = 0.0639356154273072\n",
      "Iteration 535: Loss = 0.06393296947091597\n",
      "Iteration 536: Loss = 0.06393034361983797\n",
      "Iteration 537: Loss = 0.06392773766963684\n",
      "Iteration 538: Loss = 0.06392515141808582\n",
      "Iteration 539: Loss = 0.0639225846651436\n",
      "Iteration 540: Loss = 0.06392003721293048\n",
      "Iteration 541: Loss = 0.06391750886570483\n",
      "Iteration 542: Loss = 0.06391499942983964\n",
      "Iteration 543: Loss = 0.06391250871379946\n",
      "Iteration 544: Loss = 0.06391003652811761\n",
      "Iteration 545: Loss = 0.06390758268537354\n",
      "Iteration 546: Loss = 0.06390514700017053\n",
      "Iteration 547: Loss = 0.06390272928911349\n",
      "Iteration 548: Loss = 0.06390032937078727\n",
      "Iteration 549: Loss = 0.06389794706573491\n",
      "Iteration 550: Loss = 0.06389558219643635\n",
      "Iteration 551: Loss = 0.06389323458728711\n",
      "Iteration 552: Loss = 0.06389090406457769\n",
      "Iteration 553: Loss = 0.06388859045647254\n",
      "Iteration 554: Loss = 0.06388629359298982\n",
      "Iteration 555: Loss = 0.06388401330598115\n",
      "Iteration 556: Loss = 0.06388174942911144\n",
      "Iteration 557: Loss = 0.06387950179783929\n",
      "Iteration 558: Loss = 0.06387727024939727\n",
      "Iteration 559: Loss = 0.06387505462277264\n",
      "Iteration 560: Loss = 0.06387285475868813\n",
      "Iteration 561: Loss = 0.06387067049958306\n",
      "Iteration 562: Loss = 0.06386850168959456\n",
      "Iteration 563: Loss = 0.06386634817453903\n",
      "Iteration 564: Loss = 0.06386420980189385\n",
      "Iteration 565: Loss = 0.06386208642077926\n",
      "Iteration 566: Loss = 0.06385997788194034\n",
      "Iteration 567: Loss = 0.0638578840377294\n",
      "Iteration 568: Loss = 0.06385580474208835\n",
      "Iteration 569: Loss = 0.06385373985053139\n",
      "Iteration 570: Loss = 0.0638516892201278\n",
      "Iteration 571: Loss = 0.06384965270948507\n",
      "Iteration 572: Loss = 0.06384763017873205\n",
      "Iteration 573: Loss = 0.06384562148950229\n",
      "Iteration 574: Loss = 0.06384362650491776\n",
      "Iteration 575: Loss = 0.0638416450895725\n",
      "Iteration 576: Loss = 0.06383967710951657\n",
      "Iteration 577: Loss = 0.06383772243224022\n",
      "Iteration 578: Loss = 0.06383578092665813\n",
      "Iteration 579: Loss = 0.06383385246309387\n",
      "Iteration 580: Loss = 0.06383193691326451\n",
      "Iteration 581: Loss = 0.0638300341502655\n",
      "Iteration 582: Loss = 0.06382814404855557\n",
      "Iteration 583: Loss = 0.06382626648394178\n",
      "Iteration 584: Loss = 0.06382440133356501\n",
      "Iteration 585: Loss = 0.0638225484758852\n",
      "Iteration 586: Loss = 0.06382070779066709\n",
      "Iteration 587: Loss = 0.06381887915896599\n",
      "Iteration 588: Loss = 0.06381706246311357\n",
      "Iteration 589: Loss = 0.06381525758670413\n",
      "Iteration 590: Loss = 0.06381346441458066\n",
      "Iteration 591: Loss = 0.06381168283282133\n",
      "Iteration 592: Loss = 0.06380991272872598\n",
      "Iteration 593: Loss = 0.06380815399080272\n",
      "Iteration 594: Loss = 0.06380640650875494\n",
      "Iteration 595: Loss = 0.0638046701734681\n",
      "Iteration 596: Loss = 0.06380294487699686\n",
      "Iteration 597: Loss = 0.06380123051255249\n",
      "Iteration 598: Loss = 0.06379952697449001\n",
      "Iteration 599: Loss = 0.06379783415829593\n",
      "Iteration 600: Loss = 0.0637961519605758\n",
      "Iteration 601: Loss = 0.06379448027904203\n",
      "Iteration 602: Loss = 0.06379281901250183\n",
      "Iteration 603: Loss = 0.06379116806084531\n",
      "Iteration 604: Loss = 0.0637895273250336\n",
      "Iteration 605: Loss = 0.06378789670708718\n",
      "Iteration 606: Loss = 0.06378627611007444\n",
      "Iteration 607: Loss = 0.0637846654381001\n",
      "Iteration 608: Loss = 0.06378306459629404\n",
      "Iteration 609: Loss = 0.06378147349080006\n",
      "Iteration 610: Loss = 0.06377989202876483\n",
      "Iteration 611: Loss = 0.063778320118327\n",
      "Iteration 612: Loss = 0.06377675766860633\n",
      "Iteration 613: Loss = 0.06377520458969305\n",
      "Iteration 614: Loss = 0.06377366079263724\n",
      "Iteration 615: Loss = 0.06377212618943842\n",
      "Iteration 616: Loss = 0.06377060069303513\n",
      "Iteration 617: Loss = 0.06376908421729474\n",
      "Iteration 618: Loss = 0.0637675766770034\n",
      "Iteration 619: Loss = 0.06376607798785588\n",
      "Iteration 620: Loss = 0.06376458806644576\n",
      "Iteration 621: Loss = 0.06376310683025566\n",
      "Iteration 622: Loss = 0.06376163419764744\n",
      "Iteration 623: Loss = 0.0637601700878528\n",
      "Iteration 624: Loss = 0.06375871442096359\n",
      "Iteration 625: Loss = 0.06375726711792265\n",
      "Iteration 626: Loss = 0.06375582810051438\n",
      "Iteration 627: Loss = 0.06375439729135562\n",
      "Iteration 628: Loss = 0.06375297461388664\n",
      "Iteration 629: Loss = 0.06375155999236212\n",
      "Iteration 630: Loss = 0.06375015335184225\n",
      "Iteration 631: Loss = 0.06374875461818401\n",
      "Iteration 632: Loss = 0.06374736371803247\n",
      "Iteration 633: Loss = 0.06374598057881224\n",
      "Iteration 634: Loss = 0.06374460512871892\n",
      "Iteration 635: Loss = 0.06374323729671075\n",
      "Iteration 636: Loss = 0.06374187701250027\n",
      "Iteration 637: Loss = 0.06374052420654616\n",
      "Iteration 638: Loss = 0.06373917881004507\n",
      "Iteration 639: Loss = 0.06373784075492363\n",
      "Iteration 640: Loss = 0.0637365099738304\n",
      "Iteration 641: Loss = 0.06373518640012815\n",
      "Iteration 642: Loss = 0.06373386996788602\n",
      "Iteration 643: Loss = 0.0637325606118718\n",
      "Iteration 644: Loss = 0.06373125826754435\n",
      "Iteration 645: Loss = 0.06372996287104611\n",
      "Iteration 646: Loss = 0.06372867435919562\n",
      "Iteration 647: Loss = 0.06372739266948019\n",
      "Iteration 648: Loss = 0.06372611774004858\n",
      "Iteration 649: Loss = 0.06372484950970385\n",
      "Iteration 650: Loss = 0.0637235879178962\n",
      "Iteration 651: Loss = 0.063722332904716\n",
      "Iteration 652: Loss = 0.06372108441088671\n",
      "Iteration 653: Loss = 0.06371984237775806\n",
      "Iteration 654: Loss = 0.06371860674729928\n",
      "Iteration 655: Loss = 0.06371737746209226\n",
      "Iteration 656: Loss = 0.06371615446532497\n",
      "Iteration 657: Loss = 0.06371493770078482\n",
      "Iteration 658: Loss = 0.06371372711285214\n",
      "Iteration 659: Loss = 0.06371252264649378\n",
      "Iteration 660: Loss = 0.06371132424725667\n",
      "Iteration 661: Loss = 0.06371013186126154\n",
      "Iteration 662: Loss = 0.0637089454351967\n",
      "Iteration 663: Loss = 0.06370776491631179\n",
      "Iteration 664: Loss = 0.06370659025241182\n",
      "Iteration 665: Loss = 0.06370542139185097\n",
      "Iteration 666: Loss = 0.06370425828352673\n",
      "Iteration 667: Loss = 0.06370310087687392\n",
      "Iteration 668: Loss = 0.06370194912185892\n",
      "Iteration 669: Loss = 0.06370080296897386\n",
      "Iteration 670: Loss = 0.06369966236923089\n",
      "Iteration 671: Loss = 0.06369852727415654\n",
      "Iteration 672: Loss = 0.06369739763578613\n",
      "Iteration 673: Loss = 0.06369627340665825\n",
      "Iteration 674: Loss = 0.06369515453980927\n",
      "Iteration 675: Loss = 0.06369404098876794\n",
      "Iteration 676: Loss = 0.06369293270755008\n",
      "Iteration 677: Loss = 0.0636918296506532\n",
      "Iteration 678: Loss = 0.06369073177305137\n",
      "Iteration 679: Loss = 0.06368963903018995\n",
      "Iteration 680: Loss = 0.06368855137798052\n",
      "Iteration 681: Loss = 0.06368746877279584\n",
      "Iteration 682: Loss = 0.06368639117146477\n",
      "Iteration 683: Loss = 0.06368531853126738\n",
      "Iteration 684: Loss = 0.06368425080993\n",
      "Iteration 685: Loss = 0.06368318796562045\n",
      "Iteration 686: Loss = 0.06368212995694314\n",
      "Iteration 687: Loss = 0.06368107674293443\n",
      "Iteration 688: Loss = 0.06368002828305787\n",
      "Iteration 689: Loss = 0.06367898453719958\n",
      "Iteration 690: Loss = 0.06367794546566374\n",
      "Iteration 691: Loss = 0.06367691102916789\n",
      "Iteration 692: Loss = 0.0636758811888386\n",
      "Iteration 693: Loss = 0.06367485590620697\n",
      "Iteration 694: Loss = 0.06367383514320421\n",
      "Iteration 695: Loss = 0.06367281886215735\n",
      "Iteration 696: Loss = 0.06367180702578491\n",
      "Iteration 697: Loss = 0.06367079959719268\n",
      "Iteration 698: Loss = 0.06366979653986951\n",
      "Iteration 699: Loss = 0.06366879781768311\n",
      "Iteration 700: Loss = 0.06366780339487603\n",
      "Iteration 701: Loss = 0.06366681323606153\n",
      "Iteration 702: Loss = 0.06366582730621959\n",
      "Iteration 703: Loss = 0.06366484557069285\n",
      "Iteration 704: Loss = 0.06366386799518282\n",
      "Iteration 705: Loss = 0.06366289454574592\n",
      "Iteration 706: Loss = 0.06366192518878956\n",
      "Iteration 707: Loss = 0.06366095989106846\n",
      "Iteration 708: Loss = 0.0636599986196808\n",
      "Iteration 709: Loss = 0.06365904134206456\n",
      "Iteration 710: Loss = 0.06365808802599376\n",
      "Iteration 711: Loss = 0.0636571386395749\n",
      "Iteration 712: Loss = 0.0636561931512433\n",
      "Iteration 713: Loss = 0.06365525152975954\n",
      "Iteration 714: Loss = 0.06365431374420598\n",
      "Iteration 715: Loss = 0.06365337976398325\n",
      "Iteration 716: Loss = 0.06365244955880678\n",
      "Iteration 717: Loss = 0.06365152309870341\n",
      "Iteration 718: Loss = 0.06365060035400807\n",
      "Iteration 719: Loss = 0.06364968129536032\n",
      "Iteration 720: Loss = 0.06364876589370114\n",
      "Iteration 721: Loss = 0.06364785412026971\n",
      "Iteration 722: Loss = 0.06364694594660004\n",
      "Iteration 723: Loss = 0.06364604134451793\n",
      "Iteration 724: Loss = 0.06364514028613769\n",
      "Iteration 725: Loss = 0.06364424274385909\n",
      "Iteration 726: Loss = 0.06364334869036428\n",
      "Iteration 727: Loss = 0.06364245809861467\n",
      "Iteration 728: Loss = 0.06364157094184797\n",
      "Iteration 729: Loss = 0.06364068719357516\n",
      "Iteration 730: Loss = 0.06363980682757753\n",
      "Iteration 731: Loss = 0.06363892981790385\n",
      "Iteration 732: Loss = 0.06363805613886735\n",
      "Iteration 733: Loss = 0.06363718576504289\n",
      "Iteration 734: Loss = 0.06363631867126422\n",
      "Iteration 735: Loss = 0.06363545483262108\n",
      "Iteration 736: Loss = 0.06363459422445651\n",
      "Iteration 737: Loss = 0.063633736822364\n",
      "Iteration 738: Loss = 0.06363288260218493\n",
      "Iteration 739: Loss = 0.06363203154000578\n",
      "Iteration 740: Loss = 0.06363118361215551\n",
      "Iteration 741: Loss = 0.06363033879520302\n",
      "Iteration 742: Loss = 0.0636294970659544\n",
      "Iteration 743: Loss = 0.0636286584014505\n",
      "Iteration 744: Loss = 0.06362782277896432\n",
      "Iteration 745: Loss = 0.06362699017599863\n",
      "Iteration 746: Loss = 0.06362616057028327\n",
      "Iteration 747: Loss = 0.0636253339397729\n",
      "Iteration 748: Loss = 0.06362451026264447\n",
      "Iteration 749: Loss = 0.06362368951729487\n",
      "Iteration 750: Loss = 0.06362287168233847\n",
      "Iteration 751: Loss = 0.06362205673660494\n",
      "Iteration 752: Loss = 0.06362124465913672\n",
      "Iteration 753: Loss = 0.06362043542918691\n",
      "Iteration 754: Loss = 0.06361962902621686\n",
      "Iteration 755: Loss = 0.06361882542989403\n",
      "Iteration 756: Loss = 0.06361802462008967\n",
      "Iteration 757: Loss = 0.0636172265768767\n",
      "Iteration 758: Loss = 0.06361643128052752\n",
      "Iteration 759: Loss = 0.06361563871151182\n",
      "Iteration 760: Loss = 0.0636148488504945\n",
      "Iteration 761: Loss = 0.06361406167833353\n",
      "Iteration 762: Loss = 0.0636132771760779\n",
      "Iteration 763: Loss = 0.06361249532496556\n",
      "Iteration 764: Loss = 0.06361171610642136\n",
      "Iteration 765: Loss = 0.06361093950205504\n",
      "Iteration 766: Loss = 0.0636101654936593\n",
      "Iteration 767: Loss = 0.06360939406320773\n",
      "Iteration 768: Loss = 0.06360862519285294\n",
      "Iteration 769: Loss = 0.0636078588649246\n",
      "Iteration 770: Loss = 0.06360709506192753\n",
      "Iteration 771: Loss = 0.06360633376653986\n",
      "Iteration 772: Loss = 0.06360557496161112\n",
      "Iteration 773: Loss = 0.06360481863016039\n",
      "Iteration 774: Loss = 0.06360406475537453\n",
      "Iteration 775: Loss = 0.0636033133206063\n",
      "Iteration 776: Loss = 0.06360256430937267\n",
      "Iteration 777: Loss = 0.06360181770535293\n",
      "Iteration 778: Loss = 0.06360107349238713\n",
      "Iteration 779: Loss = 0.06360033165447412\n",
      "Iteration 780: Loss = 0.06359959217577006\n",
      "Iteration 781: Loss = 0.06359885504058659\n",
      "Iteration 782: Loss = 0.06359812023338918\n",
      "Iteration 783: Loss = 0.06359738773879559\n",
      "Iteration 784: Loss = 0.06359665754157404\n",
      "Iteration 785: Loss = 0.06359592962664179\n",
      "Iteration 786: Loss = 0.06359520397906343\n",
      "Iteration 787: Loss = 0.06359448058404929\n",
      "Iteration 788: Loss = 0.06359375942695394\n",
      "Iteration 789: Loss = 0.06359304049327462\n",
      "Iteration 790: Loss = 0.06359232376864968\n",
      "Iteration 791: Loss = 0.06359160923885708\n",
      "Iteration 792: Loss = 0.06359089688981294\n",
      "Iteration 793: Loss = 0.06359018670756997\n",
      "Iteration 794: Loss = 0.06358947867831612\n",
      "Iteration 795: Loss = 0.06358877278837301\n",
      "Iteration 796: Loss = 0.06358806902419457\n",
      "Iteration 797: Loss = 0.06358736737236562\n",
      "Iteration 798: Loss = 0.06358666781960047\n",
      "Iteration 799: Loss = 0.06358597035274148\n",
      "Iteration 800: Loss = 0.06358527495875776\n",
      "Iteration 801: Loss = 0.06358458162474377\n",
      "Iteration 802: Loss = 0.063583890337918\n",
      "Iteration 803: Loss = 0.06358320108562164\n",
      "Iteration 804: Loss = 0.06358251385531725\n",
      "Iteration 805: Loss = 0.06358182863458745\n",
      "Iteration 806: Loss = 0.06358114541113372\n",
      "Iteration 807: Loss = 0.06358046417277502\n",
      "Iteration 808: Loss = 0.06357978490744659\n",
      "Iteration 809: Loss = 0.06357910760319875\n",
      "Iteration 810: Loss = 0.06357843224819557\n",
      "Iteration 811: Loss = 0.06357775883071372\n",
      "Iteration 812: Loss = 0.06357708733914126\n",
      "Iteration 813: Loss = 0.06357641776197645\n",
      "Iteration 814: Loss = 0.06357575008782657\n",
      "Iteration 815: Loss = 0.06357508430540675\n",
      "Iteration 816: Loss = 0.06357442040353883\n",
      "Iteration 817: Loss = 0.06357375837115022\n",
      "Iteration 818: Loss = 0.06357309819727278\n",
      "Iteration 819: Loss = 0.06357243987104166\n",
      "Iteration 820: Loss = 0.06357178338169432\n",
      "Iteration 821: Loss = 0.06357112871856926\n",
      "Iteration 822: Loss = 0.0635704758711051\n",
      "Iteration 823: Loss = 0.06356982482883947\n",
      "Iteration 824: Loss = 0.06356917558140787\n",
      "Iteration 825: Loss = 0.06356852811854279\n",
      "Iteration 826: Loss = 0.0635678824300725\n",
      "Iteration 827: Loss = 0.06356723850592017\n",
      "Iteration 828: Loss = 0.0635665963361028\n",
      "Iteration 829: Loss = 0.06356595591073025\n",
      "Iteration 830: Loss = 0.06356531722000418\n",
      "Iteration 831: Loss = 0.06356468025421716\n",
      "Iteration 832: Loss = 0.06356404500375169\n",
      "Iteration 833: Loss = 0.0635634114590792\n",
      "Iteration 834: Loss = 0.06356277961075912\n",
      "Iteration 835: Loss = 0.063562149449438\n",
      "Iteration 836: Loss = 0.06356152096584851\n",
      "Iteration 837: Loss = 0.06356089415080855\n",
      "Iteration 838: Loss = 0.06356026899522041\n",
      "Iteration 839: Loss = 0.06355964549006975\n",
      "Iteration 840: Loss = 0.06355902362642482\n",
      "Iteration 841: Loss = 0.06355840339543557\n",
      "Iteration 842: Loss = 0.06355778478833271\n",
      "Iteration 843: Loss = 0.06355716779642698\n",
      "Iteration 844: Loss = 0.06355655241110815\n",
      "Iteration 845: Loss = 0.06355593862384434\n",
      "Iteration 846: Loss = 0.06355532642618109\n",
      "Iteration 847: Loss = 0.06355471580974052\n",
      "Iteration 848: Loss = 0.06355410676622066\n",
      "Iteration 849: Loss = 0.06355349928739445\n",
      "Iteration 850: Loss = 0.06355289336510912\n",
      "Iteration 851: Loss = 0.06355228899128534\n",
      "Iteration 852: Loss = 0.06355168615791638\n",
      "Iteration 853: Loss = 0.06355108485706748\n",
      "Iteration 854: Loss = 0.06355048508087496\n",
      "Iteration 855: Loss = 0.06354988682154554\n",
      "Iteration 856: Loss = 0.0635492900713556\n",
      "Iteration 857: Loss = 0.06354869482265038\n",
      "Iteration 858: Loss = 0.06354810106784335\n",
      "Iteration 859: Loss = 0.06354750879941543\n",
      "Iteration 860: Loss = 0.06354691800991427\n",
      "Iteration 861: Loss = 0.06354632869195358\n",
      "Iteration 862: Loss = 0.06354574083821242\n",
      "Iteration 863: Loss = 0.06354515444143453\n",
      "Iteration 864: Loss = 0.06354456949442762\n",
      "Iteration 865: Loss = 0.06354398599006268\n",
      "Iteration 866: Loss = 0.06354340392127342\n",
      "Iteration 867: Loss = 0.06354282328105545\n",
      "Iteration 868: Loss = 0.06354224406246582\n",
      "Iteration 869: Loss = 0.06354166625862215\n",
      "Iteration 870: Loss = 0.0635410898627022\n",
      "Iteration 871: Loss = 0.06354051486794311\n",
      "Iteration 872: Loss = 0.0635399412676409\n",
      "Iteration 873: Loss = 0.06353936905514962\n",
      "Iteration 874: Loss = 0.06353879822388106\n",
      "Iteration 875: Loss = 0.06353822876730385\n",
      "Iteration 876: Loss = 0.06353766067894309\n",
      "Iteration 877: Loss = 0.06353709395237962\n",
      "Iteration 878: Loss = 0.06353652858124945\n",
      "Iteration 879: Loss = 0.06353596455924328\n",
      "Iteration 880: Loss = 0.06353540188010579\n",
      "Iteration 881: Loss = 0.0635348405376352\n",
      "Iteration 882: Loss = 0.06353428052568263\n",
      "Iteration 883: Loss = 0.06353372183815159\n",
      "Iteration 884: Loss = 0.06353316446899739\n",
      "Iteration 885: Loss = 0.06353260841222666\n",
      "Iteration 886: Loss = 0.06353205366189674\n",
      "Iteration 887: Loss = 0.06353150021211523\n",
      "Iteration 888: Loss = 0.06353094805703939\n",
      "Iteration 889: Loss = 0.06353039719087572\n",
      "Iteration 890: Loss = 0.06352984760787933\n",
      "Iteration 891: Loss = 0.06352929930235351\n",
      "Iteration 892: Loss = 0.06352875226864922\n",
      "Iteration 893: Loss = 0.06352820650116457\n",
      "Iteration 894: Loss = 0.06352766199434436\n",
      "Iteration 895: Loss = 0.06352711874267955\n",
      "Iteration 896: Loss = 0.06352657674070684\n",
      "Iteration 897: Loss = 0.06352603598300817\n",
      "Iteration 898: Loss = 0.06352549646421021\n",
      "Iteration 899: Loss = 0.06352495817898396\n",
      "Iteration 900: Loss = 0.06352442112204425\n",
      "Iteration 901: Loss = 0.06352388528814931\n",
      "Iteration 902: Loss = 0.06352335067210028\n",
      "Iteration 903: Loss = 0.06352281726874083\n",
      "Iteration 904: Loss = 0.06352228507295667\n",
      "Iteration 905: Loss = 0.06352175407967511\n",
      "Iteration 906: Loss = 0.0635212242838647\n",
      "Iteration 907: Loss = 0.06352069568053466\n",
      "Iteration 908: Loss = 0.06352016826473468\n",
      "Iteration 909: Loss = 0.06351964203155426\n",
      "Iteration 910: Loss = 0.06351911697612247\n",
      "Iteration 911: Loss = 0.06351859309360751\n",
      "Iteration 912: Loss = 0.06351807037921624\n",
      "Iteration 913: Loss = 0.06351754882819381\n",
      "Iteration 914: Loss = 0.06351702843582337\n",
      "Iteration 915: Loss = 0.06351650919742548\n",
      "Iteration 916: Loss = 0.0635159911083579\n",
      "Iteration 917: Loss = 0.06351547416401516\n",
      "Iteration 918: Loss = 0.06351495835982812\n",
      "Iteration 919: Loss = 0.06351444369126366\n",
      "Iteration 920: Loss = 0.0635139301538243\n",
      "Iteration 921: Loss = 0.06351341774304782\n",
      "Iteration 922: Loss = 0.06351290645450691\n",
      "Iteration 923: Loss = 0.06351239628380882\n",
      "Iteration 924: Loss = 0.06351188722659498\n",
      "Iteration 925: Loss = 0.06351137927854068\n",
      "Iteration 926: Loss = 0.06351087243535469\n",
      "Iteration 927: Loss = 0.06351036669277896\n",
      "Iteration 928: Loss = 0.06350986204658823\n",
      "Iteration 929: Loss = 0.0635093584925898\n",
      "Iteration 930: Loss = 0.06350885602662301\n",
      "Iteration 931: Loss = 0.0635083546445591\n",
      "Iteration 932: Loss = 0.0635078543423008\n",
      "Iteration 933: Loss = 0.06350735511578204\n",
      "Iteration 934: Loss = 0.06350685696096754\n",
      "Iteration 935: Loss = 0.06350635987385264\n",
      "Iteration 936: Loss = 0.0635058638504629\n",
      "Iteration 937: Loss = 0.06350536888685382\n",
      "Iteration 938: Loss = 0.06350487497911049\n",
      "Iteration 939: Loss = 0.06350438212334739\n",
      "Iteration 940: Loss = 0.063503890315708\n",
      "Iteration 941: Loss = 0.06350339955236448\n",
      "Iteration 942: Loss = 0.06350290982951756\n",
      "Iteration 943: Loss = 0.06350242114339603\n",
      "Iteration 944: Loss = 0.0635019334902566\n",
      "Iteration 945: Loss = 0.06350144686638354\n",
      "Iteration 946: Loss = 0.06350096126808845\n",
      "Iteration 947: Loss = 0.06350047669171\n",
      "Iteration 948: Loss = 0.06349999313361357\n",
      "Iteration 949: Loss = 0.06349951059019109\n",
      "Iteration 950: Loss = 0.06349902905786067\n",
      "Iteration 951: Loss = 0.06349854853306643\n",
      "Iteration 952: Loss = 0.06349806901227815\n",
      "Iteration 953: Loss = 0.06349759049199113\n",
      "Iteration 954: Loss = 0.0634971129687258\n",
      "Iteration 955: Loss = 0.06349663643902755\n",
      "Iteration 956: Loss = 0.06349616089946646\n",
      "Iteration 957: Loss = 0.06349568634663702\n",
      "Iteration 958: Loss = 0.06349521277715801\n",
      "Iteration 959: Loss = 0.06349474018767207\n",
      "Iteration 960: Loss = 0.0634942685748456\n",
      "Iteration 961: Loss = 0.06349379793536847\n",
      "Iteration 962: Loss = 0.06349332826595382\n",
      "Iteration 963: Loss = 0.06349285956333779\n",
      "Iteration 964: Loss = 0.06349239182427928\n",
      "Iteration 965: Loss = 0.0634919250455598\n",
      "Iteration 966: Loss = 0.06349145922398317\n",
      "Iteration 967: Loss = 0.0634909943563753\n",
      "Iteration 968: Loss = 0.06349053043958405\n",
      "Iteration 969: Loss = 0.06349006747047889\n",
      "Iteration 970: Loss = 0.06348960544595084\n",
      "Iteration 971: Loss = 0.06348914436291209\n",
      "Iteration 972: Loss = 0.06348868421829587\n",
      "Iteration 973: Loss = 0.06348822500905633\n",
      "Iteration 974: Loss = 0.06348776673216815\n",
      "Iteration 975: Loss = 0.0634873093846265\n",
      "Iteration 976: Loss = 0.06348685296344671\n",
      "Iteration 977: Loss = 0.06348639746566422\n",
      "Iteration 978: Loss = 0.06348594288833419\n",
      "Iteration 979: Loss = 0.06348548922853152\n",
      "Iteration 980: Loss = 0.06348503648335044\n",
      "Iteration 981: Loss = 0.06348458464990454\n",
      "Iteration 982: Loss = 0.06348413372532638\n",
      "Iteration 983: Loss = 0.06348368370676745\n",
      "Iteration 984: Loss = 0.0634832345913979\n",
      "Iteration 985: Loss = 0.06348278637640639\n",
      "Iteration 986: Loss = 0.0634823390589999\n",
      "Iteration 987: Loss = 0.0634818926364036\n",
      "Iteration 988: Loss = 0.06348144710586055\n",
      "Iteration 989: Loss = 0.0634810024646317\n",
      "Iteration 990: Loss = 0.06348055870999555\n",
      "Iteration 991: Loss = 0.06348011583924808\n",
      "Iteration 992: Loss = 0.06347967384970254\n",
      "Iteration 993: Loss = 0.06347923273868933\n",
      "Iteration 994: Loss = 0.06347879250355576\n",
      "Iteration 995: Loss = 0.06347835314166592\n",
      "Iteration 996: Loss = 0.06347791465040055\n",
      "Iteration 997: Loss = 0.06347747702715686\n",
      "Iteration 998: Loss = 0.06347704026934835\n",
      "Iteration 999: Loss = 0.06347660437440462\n",
      "Final parameters: [[0.40816378]\n",
      " [0.42402298]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\666\\AppData\\Local\\Temp\\ipykernel_6896\\1274042964.py:27: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  cost_history[i] = compute_loss(X, y, theta)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 假设输入数据 X, 真实标签 y\n",
    "# X: (m, n) 维的训练数据，m 为样本数量，n 为特征维度\n",
    "# y: (m, 1) 维的目标标签\n",
    "# 这里我们假设用简单的线性模型: y = X @ theta\n",
    "\n",
    "def compute_loss(X, y, theta):\n",
    "    \"\"\"计算二次损失函数\"\"\"\n",
    "    m = len(y)  # 样本数量\n",
    "    predictions = X @ theta\n",
    "    error = predictions - y\n",
    "    cost = (1 / (2 * m)) * np.dot(error.T, error)\n",
    "    return cost\n",
    "\n",
    "def gradient_descent(X, y, theta, learning_rate, iterations):\n",
    "    \"\"\"梯度下降算法\"\"\"\n",
    "    m = len(y)  # 样本数量\n",
    "    cost_history = np.zeros(iterations)  # 用来记录每次迭代的损失\n",
    "\n",
    "    for i in range(iterations):\n",
    "        # 计算梯度\n",
    "        gradients = (1/m) * X.T @ (X @ theta - y)\n",
    "        # 更新参数 theta\n",
    "        theta = theta - learning_rate * gradients\n",
    "        # 记录损失\n",
    "        cost_history[i] = compute_loss(X, y, theta)\n",
    "\n",
    "        # 每10次打印一次损失\n",
    "        print(f\"Iteration {i}: Loss = {cost_history[i]}\")\n",
    "    \n",
    "    return theta, cost_history\n",
    "\n",
    "# 初始化\n",
    "X = np.random.rand(100, 2)  # 100个样本，每个样本2个特征\n",
    "y = np.random.rand(100, 1)  # 目标值\n",
    "theta = np.random.rand(2, 1)  # 初始模型参数\n",
    "learning_rate = 0.01\n",
    "iterations = 1000\n",
    "\n",
    "# 运行梯度下降\n",
    "theta, cost_history = gradient_descent(X, y, theta, learning_rate, iterations)\n",
    "print(\"Final parameters:\", theta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\666\\AppData\\Local\\Temp\\ipykernel_6896\\3002569115.py:34: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  cost_history[i] = compute_loss(X, y, theta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Loss = 0.07334610883591552\n",
      "Iteration 10: Loss = 0.06893335603693941\n",
      "Iteration 20: Loss = 0.06751932542030001\n",
      "Iteration 30: Loss = 0.06676312514930464\n",
      "Iteration 40: Loss = 0.06623865500060395\n",
      "Iteration 50: Loss = 0.06584622195782941\n",
      "Iteration 60: Loss = 0.06554700988655096\n",
      "Iteration 70: Loss = 0.0653179009530054\n",
      "Iteration 80: Loss = 0.06514237914273525\n",
      "Iteration 90: Loss = 0.0650080268115592\n",
      "Iteration 100: Loss = 0.06490508429862413\n",
      "Iteration 110: Loss = 0.06482615981897144\n",
      "Iteration 120: Loss = 0.06476573045591304\n",
      "Iteration 130: Loss = 0.0647194445108222\n",
      "Iteration 140: Loss = 0.06468393825859352\n",
      "Iteration 150: Loss = 0.06465675122383925\n",
      "Iteration 160: Loss = 0.0646359270974954\n",
      "Iteration 170: Loss = 0.06461998454726209\n",
      "Iteration 180: Loss = 0.064607748277126\n",
      "Iteration 190: Loss = 0.06459840373885363\n",
      "Iteration 200: Loss = 0.06459123262615399\n",
      "Iteration 210: Loss = 0.06458575827276006\n",
      "Iteration 220: Loss = 0.0645815734779402\n",
      "Iteration 230: Loss = 0.06457833622113643\n",
      "Iteration 240: Loss = 0.06457586082574174\n",
      "Iteration 250: Loss = 0.0645739863072348\n",
      "Iteration 260: Loss = 0.0645725360655192\n",
      "Iteration 270: Loss = 0.06457141871356936\n",
      "Iteration 280: Loss = 0.06457056877305012\n",
      "Iteration 290: Loss = 0.06456991796978127\n",
      "Iteration 300: Loss = 0.06456942199024049\n",
      "Iteration 310: Loss = 0.06456904800396554\n",
      "Iteration 320: Loss = 0.06456875595662605\n",
      "Iteration 330: Loss = 0.06456852243184914\n",
      "Iteration 340: Loss = 0.06456834744270007\n",
      "Iteration 350: Loss = 0.06456821570628168\n",
      "Iteration 360: Loss = 0.06456811459672422\n",
      "Iteration 370: Loss = 0.06456803527996759\n",
      "Iteration 380: Loss = 0.06456797381671361\n",
      "Iteration 390: Loss = 0.06456792913557013\n",
      "Iteration 400: Loss = 0.06456789730243066\n",
      "Iteration 410: Loss = 0.06456786809926451\n",
      "Iteration 420: Loss = 0.06456784530933404\n",
      "Iteration 430: Loss = 0.0645678295659626\n",
      "Iteration 440: Loss = 0.06456781707603063\n",
      "Iteration 450: Loss = 0.06456780756439179\n",
      "Iteration 460: Loss = 0.06456780111261545\n",
      "Iteration 470: Loss = 0.0645677949656436\n",
      "Iteration 480: Loss = 0.06456779288812418\n",
      "Iteration 490: Loss = 0.06456779310981293\n",
      "Iteration 500: Loss = 0.0645677859352718\n",
      "Iteration 510: Loss = 0.06456778443029859\n",
      "Iteration 520: Loss = 0.06456778287452089\n",
      "Iteration 530: Loss = 0.06456778132983229\n",
      "Iteration 540: Loss = 0.06456778104767512\n",
      "Iteration 550: Loss = 0.06456778417163074\n",
      "Iteration 560: Loss = 0.06456778071809881\n",
      "Iteration 570: Loss = 0.06456777997202058\n",
      "Iteration 580: Loss = 0.06456777837335112\n",
      "Iteration 590: Loss = 0.06456777805184588\n",
      "Iteration 600: Loss = 0.06456777794458138\n",
      "Iteration 610: Loss = 0.06456778463549216\n",
      "Iteration 620: Loss = 0.06456777910253328\n",
      "Iteration 630: Loss = 0.06456777818894739\n",
      "Iteration 640: Loss = 0.06456777827407505\n",
      "Iteration 650: Loss = 0.06456777918567383\n",
      "Iteration 660: Loss = 0.06456778060810803\n",
      "Iteration 670: Loss = 0.06456778300008074\n",
      "Iteration 680: Loss = 0.06456778405881475\n",
      "Iteration 690: Loss = 0.06456777850881129\n",
      "Iteration 700: Loss = 0.06456777751279869\n",
      "Iteration 710: Loss = 0.06456778276301745\n",
      "Iteration 720: Loss = 0.06456778087415574\n",
      "Iteration 730: Loss = 0.06456777811805851\n",
      "Iteration 740: Loss = 0.06456777770782084\n",
      "Iteration 750: Loss = 0.06456777877020782\n",
      "Iteration 760: Loss = 0.06456777738137953\n",
      "Iteration 770: Loss = 0.06456777807880655\n",
      "Iteration 780: Loss = 0.06456777753322959\n",
      "Iteration 790: Loss = 0.06456777731335267\n",
      "Iteration 800: Loss = 0.06456777729171076\n",
      "Iteration 810: Loss = 0.06456777762607839\n",
      "Iteration 820: Loss = 0.0645677776260442\n",
      "Iteration 830: Loss = 0.06456777823352126\n",
      "Iteration 840: Loss = 0.06456778123570203\n",
      "Iteration 850: Loss = 0.06456777958934486\n",
      "Iteration 860: Loss = 0.06456777743957798\n",
      "Iteration 870: Loss = 0.06456777797156703\n",
      "Iteration 880: Loss = 0.06456777824982152\n",
      "Iteration 890: Loss = 0.06456777951584544\n",
      "Iteration 900: Loss = 0.0645677776109084\n",
      "Iteration 910: Loss = 0.06456777728840891\n",
      "Iteration 920: Loss = 0.06456777747465196\n",
      "Iteration 930: Loss = 0.06456777821383207\n",
      "Iteration 940: Loss = 0.06456777775690611\n",
      "Iteration 950: Loss = 0.06456777807899874\n",
      "Iteration 960: Loss = 0.06456778112231368\n",
      "Iteration 970: Loss = 0.06456777780274482\n",
      "Iteration 980: Loss = 0.06456778310084937\n",
      "Iteration 990: Loss = 0.06456777735842475\n",
      "Final parameters: [[0.44596703]\n",
      " [0.41405593]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_loss(X, y, theta):\n",
    "    \"\"\"计算二次损失函数\"\"\"\n",
    "    m = len(y)\n",
    "    predictions = X @ theta\n",
    "    error = predictions - y\n",
    "    cost = (1 / (2 * m)) * np.dot(error.T, error)\n",
    "    return cost\n",
    "\n",
    "def mini_batch_gradient_descent(X, y, theta, learning_rate, iterations, batch_size):\n",
    "    \"\"\"Mini-batch 梯度下降算法\"\"\"\n",
    "    m = len(y)\n",
    "    cost_history = np.zeros(iterations)\n",
    "    num_batches = int(m / batch_size)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        # 每轮打乱数据\n",
    "        indices = np.random.permutation(m)\n",
    "        X_shuffled = X[indices]\n",
    "        y_shuffled = y[indices]\n",
    "\n",
    "        for j in range(0, m, batch_size):\n",
    "            # 取出当前 batch\n",
    "            X_batch = X_shuffled[j:j + batch_size]\n",
    "            y_batch = y_shuffled[j:j + batch_size]\n",
    "\n",
    "            # 计算当前 batch 的梯度\n",
    "            gradients = (1 / batch_size) * X_batch.T @ (X_batch @ theta - y_batch)\n",
    "            # 更新参数\n",
    "            theta = theta - learning_rate * gradients\n",
    "\n",
    "        # 记录损失\n",
    "        cost_history[i] = compute_loss(X, y, theta)\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Iteration {i}: Loss = {cost_history[i]}\")\n",
    "    \n",
    "    return theta, cost_history\n",
    "\n",
    "# 初始化\n",
    "X = np.random.rand(1000, 2)  # 1000个样本，每个样本2个特征\n",
    "y = np.random.rand(1000, 1)  # 目标值\n",
    "theta = np.random.rand(2, 1)  # 初始模型参数\n",
    "learning_rate = 0.01\n",
    "iterations = 1000\n",
    "batch_size = 64  # 每个 batch 的大小\n",
    "\n",
    "# 运行 mini-batch 梯度下降\n",
    "theta, cost_history = mini_batch_gradient_descent(X, y, theta, learning_rate, iterations, batch_size)\n",
    "print(\"Final parameters:\", theta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
